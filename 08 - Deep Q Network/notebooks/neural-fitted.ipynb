{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fitted Q (NFQ)\n",
    "\n",
    "In many real tasks the state space is combinatorial and enormous and the problem is not just the memory needed for large tables, but the time needed to fill them accurately. It is necessary **to generalize**. Generalization from examples has already been extensively studied, we need to combine reinforcement learning methods with machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cart-pole environment is a classic in reinforcement learning. The state space is low dimensional but continuous. Training is fast, yet still somewhat challenging, and function approximation can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# It works with gym version 0.20.0\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "def show_environment(env, steps=100):\n",
    "    plt.axis('off')\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    for _ in range(steps): \n",
    "        action = env.action_space.sample()\n",
    "        _, _, done, _ = env.step(action)\n",
    "        if done: \n",
    "            observation = env.reset()\n",
    "        display.clear_output(wait=True)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        plt.show()   \n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_environment(env, steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implent a function approximator that take as input the state and provide in output the Q-values for all the actions of that state as a fully connected network using the PyTorch library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCQ(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=(32,32), activation_fc=F.relu):\n",
    "        super(FCQ, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        \n",
    "        # defining the input layer\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        \n",
    "        # creating the hidden layers\n",
    "        # notice how flexiblibility: it allows to change the number of layers \n",
    "        # and units per layer. \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            \n",
    "        # connecting the last hidden layer    \n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # take in the raw state and convert it into a tensor\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device='cpu', dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        # pass it through the input layer and then through the activation function\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        \n",
    "        # then pass for all hidden layers\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        # finally, for the output layer\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to('cpu')\n",
    "        actions = torch.from_numpy(actions).long().to('cpu')\n",
    "        new_states = torch.from_numpy(new_states).float().to('cpu')\n",
    "        rewards = torch.from_numpy(rewards).float().to('cpu')\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to('cpu')\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide how to balance the exploration-exploitation trade-off: almost any technique would work fine. To keep things simple, we’re going to use an epsilon-greedy strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyStrategy():\n",
    "    \n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            # pulling out the Q-values for state s\n",
    "            q_values = model(state).cpu().detach()\n",
    "            # make the values NumPy friendly \n",
    "            q_values = q_values.data.numpy().squeeze()\n",
    "\n",
    "        # get a random number..\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            # ...if greater than epsilon, act greedily\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            # ...otherwise, act randomly\n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when evaluating the agent, we will use the action greedy with respect to the learned action-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyStrategy():\n",
    "    \n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach()\n",
    "            q_values = q_values.data.numpy().squeeze()\n",
    "            return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full neural fitted Q-iteration (NFQ) algorithm has the following steps:\n",
    "1. collect E experiences (e.g. 1024 samples)\n",
    "2. calculate the off-policy TD targets\n",
    "3. fit the action-value function using MSE and RMSprop\n",
    "4. repeats steps 2 and 3 K number of times before going back to step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import time\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class NFQ():\n",
    "    def __init__(self, env, model, optimizer, train_strategy, eval_strategy,\n",
    "                 seed, batch_size, epochs):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_strategy = train_strategy\n",
    "        self.eval_strategy = eval_strategy\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "        \n",
    "        # get the values of the Q-function at next state \n",
    "        q_sp = self.model(next_states).detach()\n",
    "        \n",
    "        # get the max value of the next state\n",
    "        # unsqueeze adds a dimension to the vector so the \n",
    "        # operations that follow work on the correct elements\n",
    "        max_a_q_sp = q_sp.max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # one important step, often overlooked, is to ensure \n",
    "        # terminal states are grounded to zero\n",
    "        max_a_q_sp *= (1 - is_terminals)\n",
    "        \n",
    "        # calculate the target \n",
    "        target_q_sa = rewards + self.gamma * max_a_q_sp\n",
    "        \n",
    "        # finally, we get the current estimate of Q(s,a)\n",
    "        q_sa = self.model(states).gather(1, actions)\n",
    "\n",
    "        # create the errors\n",
    "        td_errors = q_sa - target_q_sa\n",
    "        \n",
    "        # calculate the loss, and optimize the online network\n",
    "        value_loss = td_errors.pow(2).mul(0.5).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state):\n",
    "        action = self.train_strategy.select_action(self.model, state)\n",
    "        \n",
    "        # collect an experience tuple as usual\n",
    "        new_state, reward, is_terminal, info = self.env.step(action)\n",
    "        \n",
    "        # check for the key TimeLimit.truncated\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "        \n",
    "        # so a failure is defined as follows\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        \n",
    "        # add the terminal flag if the episode ended in failure\n",
    "        # if it isn’t a failure, we want to bootstrap on the value of the new_state \n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "        self.experiences.append(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += int(self.train_strategy.exploratory_action_taken)\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    def train(self, gamma, max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start = time.time()\n",
    "        last_debug_time = float('-inf')\n",
    "\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "    \n",
    "        self.experiences = []\n",
    "\n",
    "        result = np.empty((max_episodes, 4))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = self.env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.interaction_step(state)\n",
    "                \n",
    "                if len(self.experiences) >= self.batch_size:\n",
    "                    experiences = np.array(self.experiences, dtype=object)\n",
    "                    batches = [np.vstack(sars) for sars in experiences.T]\n",
    "                    experiences = self.model.load(batches)\n",
    "                    for _ in range(self.epochs):\n",
    "                        self.optimize_model(experiences)\n",
    "                    self.experiences.clear()\n",
    "                \n",
    "                if is_terminal:\n",
    "                    break\n",
    "            \n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            evaluation_score, _ = self.evaluate()\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_100_train_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_train_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "\n",
    "            result[episode-1] = total_step, mean_100_train_reward, mean_100_eval_score, training_time\n",
    "            \n",
    "            total_elapsed = time.time() - training_start\n",
    "            reached_debug_time = time.time() - last_debug_time >= 60\n",
    "            reached_max_minutes = total_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or reached_max_episodes or reached_goal_mean_reward\n",
    "\n",
    "            debug_message = 'episode {:04}, steps {:06}, '\n",
    "            debug_message += 'avg score {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message = debug_message.format(episode-1, total_step, mean_100_train_reward, std_100_train_reward)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            \n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            \n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print('--> reached_max_minutes')\n",
    "                if reached_max_episodes: print('--> reached_max_episodes')\n",
    "                if reached_goal_mean_reward: print('--> reached_goal_mean_reward')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, final_eval_std = self.evaluate(n_episodes=100)\n",
    "        \n",
    "        return result, final_eval_score, final_eval_std, training_time\n",
    "    \n",
    "    def evaluate(self, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = self.env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = self.eval_strategy.select_action(self.model, s)\n",
    "                s, r, d, _ = self.env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the NFQ algorithms to the Cart-pole environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "episode 0000, steps 000013, avg score 013.0±000.0, \n",
      "episode 0544, steps 010297, avg score 032.6±020.9, \n",
      "episode 0618, steps 015122, avg score 061.3±039.9, \n",
      "episode 0622, steps 015350, avg score 062.4±039.5, \r"
     ]
    }
   ],
   "source": [
    "nfq_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "\n",
    "gamma = 1.00\n",
    "max_minutes = 20\n",
    "max_episodes = 10000\n",
    "goal_mean_100_reward = 475\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 40\n",
    "\n",
    "nS, nA = env.observation_space.shape[0], env.action_space.n\n",
    "\n",
    "for seed in (12, 34, 56, 78, 90):\n",
    "    \n",
    "    print('Training started...')\n",
    "    \n",
    "    model = FCQ(nS, nA, hidden_dims=(512,128))\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.0005)\n",
    "    train_strategy = EGreedyStrategy(epsilon=0.5)\n",
    "    eval_strategy = GreedyStrategy()\n",
    "    \n",
    "    agent = NFQ(env, model, optimizer, train_strategy, eval_strategy, seed, batch_size, epochs)\n",
    "    result, score, score_std, training_time = agent.train(gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    \n",
    "    print('...training complete.')\n",
    "    print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time\\n'\n",
    "          .format(score, score_std, training_time))\n",
    "    \n",
    "    nfq_results.append(result)\n",
    "    if score > best_eval_score:\n",
    "        best_eval_score = score\n",
    "        best_agent = agent\n",
    "        \n",
    "nfq_results = np.array(nfq_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can shows performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfq_max_steps, nfq_max_train_reward, nfq_max_eval_score, nfq_max_time = np.max(nfq_results, axis=0).T\n",
    "nfq_min_steps, nfq_min_train_reward, nfq_min_eval_score, nfq_min_time = np.min(nfq_results, axis=0).T\n",
    "nfq_mean_steps, nfq_mean_train_reward, nfq_mean_eval_score, nfq_mean_time = np.mean(nfq_results, axis=0).T\n",
    "nfq_episodes = np.arange(len(nfq_mean_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the training reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(nfq_max_train_reward, 'y', linewidth=1)\n",
    "plt.plot(nfq_min_train_reward, 'y', linewidth=1)\n",
    "plt.plot(nfq_mean_train_reward, 'y', linewidth=2)\n",
    "plt.fill_between(nfq_episodes, nfq_min_train_reward, nfq_max_train_reward, facecolor='y', alpha=0.3)\n",
    "plt.title('Moving Avg Reward (Training)')\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It never reaches the max of 475 reward per episode. The reason is we’re using an epsilon of 0.5. Having such a high exploration rate helps with finding more accurate value functions, but it shows worse performance during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the reward during evaluation steps shows the best performance we can obtain from the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(nfq_max_eval_score, 'y', linewidth=1)\n",
    "plt.plot(nfq_min_eval_score, 'y', linewidth=1)\n",
    "plt.plot(nfq_mean_eval_score, 'y', linewidth=2)\n",
    "plt.fill_between(nfq_episodes, nfq_min_eval_score, nfq_max_eval_score, facecolor='y', alpha=0.3)\n",
    "plt.title('Moving Avg Reward (Evaluation)')\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issue with NFQ is that it takes too many steps to get decent performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(nfq_max_steps, 'y', linewidth=1)\n",
    "plt.plot(nfq_min_steps, 'y', linewidth=1)\n",
    "plt.plot(nfq_mean_steps, 'y', linewidth=2)\n",
    "plt.fill_between(nfq_episodes, nfq_min_steps, nfq_max_steps, facecolor='y', alpha=0.3)\n",
    "plt.title('Total Steps')\n",
    "plt.ylabel('Steps')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the trained model behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "def show_policy(agent, env, steps=100):\n",
    "    plt.axis('off')\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    for _ in range(steps): \n",
    "        action = agent.eval_strategy.select_action(agent.model, observation)\n",
    "        observation, _, done, _ = env.step(action)\n",
    "        if done: \n",
    "            observation = env.reset()\n",
    "        display.clear_output(wait=True)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        plt.show()   \n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFH0lEQVR4nO3cwWqcVRjH4febjCnFcZUaAhWEugg26DIuvAcDuQjvIHgT4hV0F9wIAUHdu9CssxGMuNCgJZhQbcd0kpDJ50I3kmkmSf+dGdrnWZ4ZOO/i8OObfIc0bdsWAM+vM+0BAF4WggoQIqgAIYIKECKoACHdMZ+7AgBwUTNq0RMqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoR0pz0APMvZyaCeHv56YX1u/na9/ubbU5gILieozKzBnw9r9+vPLqz3lt6pd9c+mcJEcDk/+QFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUIEFSBEUAFCBBUgRFABQgQVIERQAUK60x6AV8tgMKidnZ1q23bsd5unf4w8oP0n/dre3r7SfgsLC7W8vHzNKeFmmjEHe/yph2vY3d2tlZWVGg6HY7/73r3FerDxUTVN87/1nZ/36+NPv7rSfuvr67W1tXWjWeESzahFT6jMtLY6dXh6tw5O36r5zkndvfXTtEeCZxJUZlhTe8f3a/fogzqvuapqa//kXs0NN6c9GIzkpRQz6/HZnf9i2q1/f2F1qj9cqB+OPpz2aDCSoDKz2nauhu3chfWz9rUpTAPjCSozq9uc1nzn5ML67c7fU5gGxhNUZlav+6jef+PbutU5qqrz6tRZLc7/Uvd73097NBjp0pdS+/v7k5qDV8Th4eGV7qBWVf1+0K8HX3xeR8Nv6q+zxeo2p3Vn/rd6/KR/5f2Oj4+dY+KWlpZGrl8a1M1Nb1PJOjg4uHJQH/UH9eV3Pz7Xfnt7e84xcRsbGyPXXexnoq5zsT/BxX5ekJEX+/0NFSBEUAFCBBUgRFABQgQVIERQAUL8tykmqtfr1draWp2fn09kv9XV1YnsA1XuoQLchHuoAC+SoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpASHfM581EpgB4CXhCBQgRVIAQQQUIEVSAEEEFCBFUgJB/AARTo5262j44AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1');\n",
    "show_policy(best_agent, env, steps=200)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "math_differential_calculus",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8aeb84091b1f1fb8d8b9efbf1e96a552fa0144c39bfbc7f744113ad2216f701d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
