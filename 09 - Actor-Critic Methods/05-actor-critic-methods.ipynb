{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Methods\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/riccardoberta/machine-learning/blob/master/06-deep-reinforcement-learning/05-actor-critic-methods.ipynb)\n",
    "\n",
    "We explore a combined class of methods that learn both policies and value functions. These methods are referred to as **actor-critic** because the policy, which selects actions, can be seen as an actor, and the value function, which evaluates policies, can be seen as a critic. Actor-critic methods often perform better than value-based or policy-based methods alone on many of the deep reinforcement learning benchmarks. \n",
    "\n",
    "1. [Asynchronous Advantage Actor-Critic (A3C)](#AsynchronousAdvantage-Actor-Critic-(A3C))\n",
    "2. [Generalized advantage estimation (GAE)](#Generalized-Advantage-Estimation-(GAE))\n",
    "3. [](#)\n",
    "4. [](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "One of the main sources of variance in DRL algorithms is how correlated and non-stationary\n",
    "online samples are. In value-based methods, we use a replay buffer to uniformly sample\n",
    "mini-batches. Unfortunately,this is limited to off-policy methods, because on-policy agents cannot reuse data generated by previous policies. In other words, every optimization step requires a fresh batch of on-policy experiences. Instead of using a replay buffer, what we can do in on-policy methods (likes the policy-based methids) is have **multiple workers** generating experience in parallel and asynchronously updating the policy and value function. Having multiple workers generating experience on multiple instances of the environment in parallel decorrelates the data used for training and reduces the variance of the algorithm.\n",
    "\n",
    "<img src=\"./images/multiple-workers.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **asynchronous advantage actor-critic (A3C)** uses concurrent actors to generate a broad set of experience samples in parallel.  Moreover, it also uses **n-step returns with bootstrapping** to learn the policy and value function.\n",
    "\n",
    "We can write it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import time\n",
    "from itertools import count\n",
    "import torch.multiprocessing as mp\n",
    "#import multiprocess as mp\n",
    "    \n",
    "class A3C():\n",
    "    def __init__(self, policy_model_fn, policy_model_max_grad_norm, policy_optimizer_fn, policy_optimizer_lr,\n",
    "                 value_model_fn, value_model_max_grad_norm, value_optimizer_fn, value_optimizer_lr, \n",
    "                 entropy_loss_weight, max_n_steps, n_workers):\n",
    "                \n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        \n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_model_max_grad_norm = value_model_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        \n",
    "        self.entropy_loss_weight = entropy_loss_weight\n",
    "        self.max_n_steps = max_n_steps\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "    # this is the work function each worker loops around in \n",
    "    # the rank parameter is used as an ID for workers\n",
    "    def work(self, rank):\n",
    "        last_debug_time = float('-inf')\n",
    "        self.stats['n_active_workers'].add_(1)\n",
    "        \n",
    "        # create a unique seed per worker: we want diverse experiences\n",
    "        local_seed = self.seed + rank\n",
    "        \n",
    "        # we create a uniquely seeded environment for each worker\n",
    "        env = self.make_env_fn(seed=local_seed)\n",
    "        torch.manual_seed(local_seed)\n",
    "        np.random.seed(local_seed)\n",
    "        random.seed(local_seed)\n",
    "\n",
    "        # create a local policy model: initialize its weights with the weights\n",
    "        # of a shared policy network. This network allow us to synchronize the agents \n",
    "        # periodically.\n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.n\n",
    "        local_policy_model = self.policy_model_fn(nS, nA)\n",
    "        local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n",
    "        \n",
    "        # do the same thing with the value model\n",
    "        local_value_model = self.value_model_fn(nS)\n",
    "        local_value_model.load_state_dict(self.shared_value_model.state_dict())\n",
    "\n",
    "        # start the training loop, until the worker is signaled to get out of it\n",
    "        global_episode_idx = self.stats['episode'].add_(1).item() - 1\n",
    "        while not self.get_out_signal: \n",
    "            episode_start = time.time()\n",
    "            \n",
    "            # reset the environment, and set the is_terminal flag to false\n",
    "            state, is_terminal = env.reset(), False\n",
    "            \n",
    "            # use n-step returns for training the policy and value functions\n",
    "            n_steps_start, total_episode_rewards = 0, 0\n",
    "            total_episode_steps, total_episode_exploration = 0, 0\n",
    "            logpas, entropies, rewards, values = [], [], [], []\n",
    "\n",
    "            # the episode loop \n",
    "            for step in count(start=1):\n",
    "                \n",
    "                # collect a step of experience\n",
    "                state, reward, is_terminal, is_truncated, is_exploratory = self.interaction_step(\n",
    "                    state, env, local_policy_model, local_value_model, \n",
    "                    logpas, entropies, rewards, values)\n",
    "\n",
    "                total_episode_steps += 1\n",
    "                total_episode_rewards += reward\n",
    "                total_episode_exploration += int(is_exploratory)\n",
    "                \n",
    "                # collect n-steps maximum. If we hit a terminal state, we stop there\n",
    "                if is_terminal or step - n_steps_start == self.max_n_steps:\n",
    "\n",
    "                    # check if the time wrapper was triggered or this is a true terminal state\n",
    "                    is_failure = is_terminal and not is_truncated\n",
    "                    \n",
    "                    # if it’s a failure, then the value of the next state is 0; otherwise, we bootstrap\n",
    "                    next_value = 0 if is_failure else local_value_model(state).detach().item()\n",
    "                    \n",
    "                    # Look! we are sneaky here: appending the next_value to the rewards, the optimization \n",
    "                    # code from VPG remains largely the same\n",
    "                    rewards.append(next_value)\n",
    "                    \n",
    "                    # optimize the model\n",
    "                    self.optimize_model(logpas, entropies, rewards, values, \n",
    "                                        local_policy_model, local_value_model)\n",
    "                    \n",
    "                    logpas, entropies, rewards, values = [], [], [], []\n",
    "                    n_steps_start = step\n",
    "\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "            # save global stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            evaluation_score, _ = self.evaluate(local_policy_model, env)\n",
    "            \n",
    "            self.stats['episode_elapsed'][global_episode_idx].add_(episode_elapsed)\n",
    "            self.stats['episode_timestep'][global_episode_idx].add_(total_episode_steps)\n",
    "            self.stats['episode_reward'][global_episode_idx].add_(total_episode_rewards)\n",
    "            self.stats['episode_exploration'][global_episode_idx].add_(total_episode_exploration/total_episode_steps)\n",
    "            self.stats['evaluation_scores'][global_episode_idx].add_(evaluation_score)\n",
    "\n",
    "            mean_100_train_reward = self.stats['episode_reward'][:global_episode_idx+1][-100:].mean().item()\n",
    "            std_100_train_reward = self.stats['episode_reward'][:global_episode_idx+1][-100:].std().item()\n",
    "            mean_100_eval_reward = self.stats['evaluation_scores'][:global_episode_idx+1][-100:].mean().item()\n",
    "            std_100_eval_reward = self.stats['evaluation_scores'][:global_episode_idx+1][-100:].std().item()\n",
    "            \n",
    "            global_n_steps = self.stats['episode_timestep'][:global_episode_idx+1].sum().item()\n",
    "            global_training_elapsed = self.stats['episode_elapsed'][:global_episode_idx+1].sum().item()\n",
    "            \n",
    "            total_elapsed = time.time() - self.training_start\n",
    "            \n",
    "            self.stats['result'][global_episode_idx][0].add_(global_n_steps)\n",
    "            self.stats['result'][global_episode_idx][1].add_(mean_100_train_reward)\n",
    "            self.stats['result'][global_episode_idx][2].add_(mean_100_eval_reward)\n",
    "            self.stats['result'][global_episode_idx][3].add_(global_training_elapsed)\n",
    "            self.stats['result'][global_episode_idx][4].add_(total_elapsed)\n",
    "\n",
    "            debug_message = 'episode {:04}, steps {:06}, '\n",
    "            debug_message += 'avg score {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message = debug_message.format(global_episode_idx, global_n_steps, mean_100_train_reward, std_100_train_reward)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "                        \n",
    "            if rank == 0:\n",
    "                print(debug_message, end='\\r', flush=True)\n",
    "                if time.time() - last_debug_time >= 60:\n",
    "                    print(debug_message, flush=True)\n",
    "                    last_debug_time = time.time()\n",
    "\n",
    "            with self.get_out_lock:\n",
    "                potential_next_global_episode_idx = self.stats['episode'].item()\n",
    "                self.reached_goal_mean_reward.add_(mean_100_eval_reward >= self.goal_mean_100_reward)\n",
    "                self.reached_max_minutes.add_(time.time() - self.training_start >= self.max_minutes * 60)\n",
    "                self.reached_max_episodes.add_(potential_next_global_episode_idx >= self.max_episodes)\n",
    "                if self.reached_max_episodes or \\\n",
    "                   self.reached_max_minutes or \\\n",
    "                   self.reached_goal_mean_reward:\n",
    "                    self.get_out_signal.add_(1)\n",
    "                    break\n",
    "                    \n",
    "                # else go work on another episode\n",
    "                global_episode_idx = self.stats['episode'].add_(1).item() - 1\n",
    "\n",
    "        while rank == 0 and self.stats['n_active_workers'].item() > 1:\n",
    "            pass\n",
    "\n",
    "        if rank == 0:\n",
    "            if self.reached_max_minutes: print(u'--> reached_max_minutes')\n",
    "            if self.reached_max_episodes: print(u'--> reached_max_episodes')\n",
    "            if self.reached_goal_mean_reward: print(u'--> reached_goal_mean_reward')\n",
    "\n",
    "        env.close() \n",
    "        del env\n",
    "        \n",
    "        self.stats['n_active_workers'].sub_(1)\n",
    "\n",
    "    def optimize_model(self, logpas, entropies, rewards, values, local_policy_model, local_value_model):\n",
    "        \n",
    "        #  get the length of the reward. Remember: rewards includes the bootstrapping value.\n",
    "        T = len(rewards)\n",
    "        \n",
    "        # calculate all discounts up to n+1.\n",
    "        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n",
    "        \n",
    "        # calculate the the n-step predicted return\n",
    "        returns = np.array([np.sum(discounts[:T-t] * rewards[t:]) for t in range(T)])\n",
    "        \n",
    "        # remove the extra elements and format the variables as expected\n",
    "        discounts = torch.FloatTensor(discounts[:-1]).unsqueeze(1)\n",
    "        returns = torch.FloatTensor(returns[:-1]).unsqueeze(1)\n",
    "\n",
    "        logpas = torch.cat(logpas)\n",
    "        entropies = torch.cat(entropies)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        # calculate the value errors as the predicted return minus the estimated values\n",
    "        value_error = returns - values\n",
    "        \n",
    "        # calculate the loss\n",
    "        policy_loss = -(discounts * value_error.detach() * logpas).mean()\n",
    "        entropy_loss = -entropies.mean()\n",
    "        loss = policy_loss + self.entropy_loss_weight * entropy_loss\n",
    "        \n",
    "        # notice we now zero the shared policy optimizer, then calculate the loss\n",
    "        self.shared_policy_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip the gradient magnitude:\n",
    "        torch.nn.utils.clip_grad_norm_(local_policy_model.parameters(), self.policy_model_max_grad_norm)\n",
    "        \n",
    "        # iterating over all local and shared policy network parameters\n",
    "        # and copy every gradient from the local to the shared model\n",
    "        for param, shared_param in zip(local_policy_model.parameters(), self.shared_policy_model.parameters()):\n",
    "            if shared_param.grad is None:\n",
    "                shared_param._grad = param.grad\n",
    "        \n",
    "        # once the gradients are copied into the shared optimizer, we run an optimization step\n",
    "        self.shared_policy_optimizer.step()\n",
    "        \n",
    "        # we load the shared model into the local model\n",
    "        local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n",
    "\n",
    "        # we do the same thing but with the state-value network\n",
    "        value_loss = value_error.pow(2).mul(0.5).mean()\n",
    "        self.shared_value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(local_value_model.parameters(), self.value_model_max_grad_norm)\n",
    "        for param, shared_param in zip(local_value_model.parameters(), self.shared_value_model.parameters()):\n",
    "            if shared_param.grad is None:\n",
    "                shared_param._grad = param.grad\n",
    "        self.shared_value_optimizer.step()\n",
    "        local_value_model.load_state_dict(self.shared_value_model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def interaction_step(state, env, local_policy_model, local_value_model,\n",
    "                         logpas, entropies, rewards, values):\n",
    "        action, is_exploratory, logpa, entropy = local_policy_model.full_pass(state)\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "\n",
    "        logpas.append(logpa)\n",
    "        entropies.append(entropy)\n",
    "        rewards.append(reward)\n",
    "        values.append(local_value_model(state))\n",
    "\n",
    "        return new_state, reward, is_terminal, is_truncated, is_exploratory\n",
    "    \n",
    "    def train(self, make_env_fn, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        \n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        self.max_minutes = max_minutes\n",
    "        self.max_episodes = max_episodes\n",
    "        self.goal_mean_100_reward = goal_mean_100_reward\n",
    "\n",
    "        env = self.make_env_fn(seed=self.seed)\n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.n\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        self.stats = {}\n",
    "        self.stats['episode'] = torch.zeros(1, dtype=torch.int).share_memory_()\n",
    "        self.stats['result'] = torch.zeros([max_episodes, 5]).share_memory_()\n",
    "        self.stats['evaluation_scores'] = torch.zeros([max_episodes]).share_memory_()\n",
    "        self.stats['episode_reward'] = torch.zeros([max_episodes]).share_memory_()\n",
    "        self.stats['episode_timestep'] = torch.zeros([max_episodes], dtype=torch.int).share_memory_()\n",
    "        self.stats['episode_exploration'] = torch.zeros([max_episodes]).share_memory_()\n",
    "        self.stats['episode_elapsed'] = torch.zeros([max_episodes]).share_memory_()\n",
    "        self.stats['n_active_workers'] = torch.zeros(1, dtype=torch.int).share_memory_()\n",
    "\n",
    "        self.shared_policy_model = self.policy_model_fn(nS, nA).share_memory()\n",
    "        self.shared_policy_optimizer = self.policy_optimizer_fn(self.shared_policy_model, self.policy_optimizer_lr)\n",
    "        self.shared_value_model = self.value_model_fn(nS).share_memory()\n",
    "        self.shared_value_optimizer = self.value_optimizer_fn(self.shared_value_model, self.value_optimizer_lr)\n",
    "\n",
    "        self.get_out_lock = mp.Lock()\n",
    "        self.get_out_signal = torch.zeros(1, dtype=torch.int).share_memory_()\n",
    "        self.reached_max_minutes = torch.zeros(1, dtype=torch.int).share_memory_() \n",
    "        self.reached_max_episodes = torch.zeros(1, dtype=torch.int).share_memory_() \n",
    "        self.reached_goal_mean_reward  = torch.zeros(1, dtype=torch.int).share_memory_() \n",
    "        self.training_start = time.time()\n",
    "        \n",
    "        workers = [mp.Process(target=self.work, args=(rank,)) for rank in range(self.n_workers)]  \n",
    "        [w.start() for w in workers]\n",
    "        [w.join() for w in workers]\n",
    "        wallclock_time = time.time() - self.training_start\n",
    "\n",
    "        final_eval_score, score_std = self.evaluate(self.shared_policy_model, env, n_episodes=100)\n",
    "        env.close()\n",
    "        del env\n",
    "\n",
    "        final_episode = self.stats['episode'].item()\n",
    "        training_time = self.stats['episode_elapsed'][:final_episode+1].sum().item()\n",
    "\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "\n",
    "        self.stats['result'] = self.stats['result'].numpy()\n",
    "        self.stats['result'][final_episode:, ...] = np.nan\n",
    "        \n",
    "        return self.stats['result'], final_eval_score, training_time, wallclock_time\n",
    "\n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1, greedy=True):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                if greedy:\n",
    "                    a = eval_policy_model.select_greedy_action(s)\n",
    "                else: \n",
    "                    a = eval_policy_model.select_action(s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we append the value of the next state, whether terminal or not, to the reward sequence. Before we were using full returns for our advantage estimates:\n",
    "\n",
    "$\\begin{align}\n",
    "A(S_t,A_t;\\phi) = G_t - V(S_t;\\phi)\n",
    "\\end{align}$\n",
    "\n",
    "Now, the reward variable contains all rewards from the partial trajectory and the state-value estimate of that last state. We can also see this as having the partial return (the sequence of rewards) and the predicted remaining return (a single-number estimate) in the same place. We should realize that this is an **n-step return**: the agent go out for n-steps collecting rewards, and then **bootstrap** after that nth state (or before if we land on a terminal state, whichever comes first):\n",
    "\n",
    "$\\begin{align}\n",
    "A(S_t,A_t;\\phi) = R_t + \\gamma R_{t+1} + ... \\gamma^n T_{t-n} + \\gamma^{n+1} V(S_{t+n+1};\\phi) - V(S_t;\\phi)\n",
    "\\end{align}$\n",
    "\n",
    "We now use this n-step advantage estimate for updating the action probabilities:\n",
    "\n",
    "$\\begin{align}\n",
    "L_\\pi(\\theta)=\\frac{1}{N}\\sum\\limits_{n=0}^{N}{\\left[\\left(A(S_t,A_t;\\phi)\\right) \\log \\pi(A_t|S_t;\\theta) + \\beta H(\\pi(S_t;\\theta))\\right]}\\end{align}$\n",
    "\n",
    "We also use the n-step return to improve the value function estimate. Notice the **bootstrapping here**. This is what makes the algorithm an **actor-critic method**:\n",
    "\n",
    "$\\begin{align}\n",
    "L_v(\\phi)=\\frac{1}{N}\\sum\\limits_{n=0}^{N}{\\left[\\left(R_t + \\gamma R_{t+1} + ... \\gamma^n T_{t-n} + \\gamma^{n+1} V(S_{t+n+1};\\phi) - V(S_t;\\phi)\\right)^2\\right]} \n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most critical aspects of A3C is that its network updates are asynchronous and lockfree. Having a shared model creates a need for a blocking mechanism to prevent workers from overwriting other updates. Interestingly, A3C uses an update style called a **Hogwild!**, which workers access to shared model with the possibility of overwriting each other's work, which is shown to not only achieve a near-optimal rate of convergence but also outperform alternative schemes that use locking by an order of magnitude: [F. Niu et al. **\"HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent\"**, NIPS 2011](https://arxiv.org/abs/1106.5730?context=cs).\n",
    "\n",
    "We need to create an Adam and RMSprop optimizer that puts internal variables into shared memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SharedAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps,  weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        \n",
    "        # We only need to call the share_memory_ method on the variables we need shared across workers\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['shared_step'] = torch.zeros(1).share_memory_()\n",
    "                state['exp_avg'] = torch.zeros_like(p.data).share_memory_()\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if weight_decay: state['weight_decay'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if amsgrad: state['max_exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n",
    "\n",
    "    # override the step function so that we can manually increment the step variable, which isn’t easily\n",
    "    # put into shared memory\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p]['steps'] = self.state[p]['shared_step'].item()\n",
    "                self.state[p]['shared_step'] += 1\n",
    "        super().step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedRMSprop(torch.optim.RMSprop):\n",
    "    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False):\n",
    "        super(SharedRMSprop, self).__init__(\n",
    "            params, lr=lr, alpha=alpha, \n",
    "            eps=eps, weight_decay=weight_decay, \n",
    "            momentum=momentum, centered=centered)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['shared_step'] = torch.zeros(1).share_memory_()\n",
    "                state['square_avg'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if weight_decay:\n",
    "                    state['weight_decay'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if momentum > 0:\n",
    "                    state['momentum_buffer'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if centered:\n",
    "                    state['grad_avg'] = torch.zeros_like(p.data).share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p]['steps'] = self.state[p]['shared_step'].item()\n",
    "                self.state[p]['shared_step'] += 1\n",
    "        super().step(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try the algorithm in the cart-pole environment. We need the two ANN architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCR(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=(32,32), activation_fc=F.relu):\n",
    "        super(FCR, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def full_pass(self, state):\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        logpa = dist.log_prob(action).unsqueeze(-1)\n",
    "        entropy = dist.entropy().unsqueeze(-1)\n",
    "        is_exploratory = action != np.argmax(logits.detach().numpy())\n",
    "        return action.item(), is_exploratory.item(), logpa, entropy\n",
    "\n",
    "    def select_action(self, state):\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        logits = self.forward(state)\n",
    "        return np.argmax(logits.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCV(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=(32,32), activation_fc=F.relu):\n",
    "        super(FCV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            \n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "def make_env(seed=None):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    if seed is not None: env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x7fda09314310>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/ipykernel_29437/1320034402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 entropy_loss_weight, max_n_steps, n_workers)\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_eval_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwallclock_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_minutes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_mean_100_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ma3c_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/ipykernel_29437/2727114572.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, make_env_fn, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mwallclock_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/ipykernel_29437/2727114572.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mwallclock_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mset_spawning_popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mForkingPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x7fda09314310>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "a3c_results = []\n",
    "best_a3c_agent, best_a3c_eval_score = None, float('-inf')\n",
    "   \n",
    "gamma = 1.00\n",
    "max_minutes = 20\n",
    "max_episodes = 10000\n",
    "goal_mean_100_reward = 475\n",
    "\n",
    "for seed in (12, 34, 56, 78, 90):\n",
    "    \n",
    "    policy_model_fn = lambda nS, nA: FCR(nS, nA, hidden_dims=(128,64))\n",
    "    policy_model_max_grad_norm = 1\n",
    "    policy_optimizer_fn = lambda net, lr: SharedAdam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0005\n",
    "\n",
    "    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,128))\n",
    "    value_model_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: SharedRMSprop(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0007\n",
    "\n",
    "    entropy_loss_weight = 0.001\n",
    "\n",
    "    max_n_steps = 50\n",
    "    n_workers = 1\n",
    "    \n",
    "    agent = A3C(policy_model_fn, policy_model_max_grad_norm, policy_optimizer_fn, policy_optimizer_lr,\n",
    "                value_model_fn, value_model_max_grad_norm, value_optimizer_fn, value_optimizer_lr,\n",
    "                entropy_loss_weight, max_n_steps, n_workers)\n",
    " \n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(make_env, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    \n",
    "    a3c_results.append(result)\n",
    "    if final_eval_score > best_a3c_eval_score:\n",
    "        best_a3c_eval_score = final_eval_score\n",
    "        best_a3c_agent = agent\n",
    "        \n",
    "a3c_results = np.array(a3c_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized advantage estimation (GAE)\n",
    "\n",
    "A3C uses n-step returns for reducing the variance of the targets. Still, there’s a more robust method that combines multiple n-step bootstrapping targets in a single target, creating even more robust targets than a single n-step: the $\\lambda$-target. **Generalized advantage estimation (GAE)** is analogous to the $\\lambda$-target in TD($\\lambda$), but for advantages. [John Schulman et al. **\"High-dimensional Continuous Control Using Generalized Advantage Estimation\"**, ICLR 2016](https://arxiv.org/abs/1506.02438) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAE is not an agent on its own, but a way of estimating targets for the advantage function that most actor-critic methods can leverage. More specifically, GAE uses an exponentially weighted combination of n-step action-advantage function targets, this can substantially reduce the variance of policy-gradient estimates at the cost of some bias. We can consider N-step advantage estimates:\n",
    "\n",
    "$\\begin{align}\n",
    "A^1(S_t,A_t;\\phi) = R_t + \\gamma V(S_{t+1};\\phi) - V(S_t;\\phi)\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "A^2(S_t,A_t;\\phi) = R_t + \\gamma R_{t+1} + \\gamma^2 V(S_{t+2};\\phi) - V(S_t;\\phi)\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "A^3(S_t,A_t;\\phi) = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 V(S_{t+3};\\phi) - V(S_t;\\phi)\n",
    "\\end{align}$\n",
    "\n",
    "...\n",
    "\n",
    "$\\begin{align}\n",
    "A^n(S_t,A_t;\\phi) = R_t + \\gamma R_{t+1} + ... + \\gamma^n R_{t+n} + \\gamma^{n+1} V(S_{t+n+1};\\phi) - V(S_t;\\phi)\n",
    "\\end{align}$\n",
    "\n",
    "which we can mix to make an estimate analogous to TD($\\lambda$), but for advantages:\n",
    "\n",
    "$\\begin{align}\n",
    "A^{\\text{GAE}(\\gamma,\\lambda)}(S_t,A_t;\\phi) = \\sum\\limits_{l=0}^{\\infty}{(\\gamma \\lambda)^l \\delta_{t+l}}\n",
    "\\end{align}$\n",
    "\n",
    "$\\lambda=0$ returns the one-step advantage estimate, and $\\lambda=1$ returns the infinite-step advantage\n",
    "estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3611165480.py, line 244)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/ipykernel_32576/3611165480.py\"\u001b[0;36m, line \u001b[0;32m244\u001b[0m\n\u001b[0;31m    //self.get_out_lock = mp.Lock()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class GAE():\n",
    "    def __init__(self, policy_model_fn, policy_model_max_grad_norm, policy_optimizer_fn, policy_optimizer_lr,\n",
    "                 value_model_fn, value_model_max_grad_norm, value_optimizer_fn, value_optimizer_lr,\n",
    "                 entropy_loss_weight, max_n_steps, n_workers, tau):\n",
    "        \n",
    "        # notice that lambda is a reserveved word, so often is referred as tau\n",
    "        \n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_model_max_grad_norm = value_model_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "\n",
    "        self.entropy_loss_weight = entropy_loss_weight\n",
    "\n",
    "        self.max_n_steps = max_n_steps\n",
    "        self.n_workers = n_workers\n",
    "        self.tau = tau\n",
    "\n",
    "    def optimize_model(self, logpas, entropies, rewards, values, \n",
    "                       local_policy_model, local_value_model):\n",
    "        \n",
    "        # create the discounted returns, the way we did with A3C\n",
    "        T = len(rewards)\n",
    "        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n",
    "        returns = np.array([np.sum(discounts[:T-t] * rewards[t:]) for t in range(T)])\n",
    "\n",
    "        logpas = torch.cat(logpas)\n",
    "        entropies = torch.cat(entropies)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        # crete an array with all the state values and an array with the (gamma*lambda)^t        \n",
    "        np_values = values.view(-1).data.numpy()\n",
    "        tau_discounts = np.logspace(0, T-1, num=T-1, base=self.gamma*self.tau, endpoint=False)\n",
    "        \n",
    "        # creates an array of TD errors: R_t + gamma * value_t+1 - value_t, for t=0 to T\n",
    "        advs = rewards[:-1] + self.gamma * np_values[1:] - np_values[:-1]  \n",
    "        \n",
    "        # create the GAEs, by multiplying the tau discounts times the TD errors\n",
    "        gaes = np.array([np.sum(tau_discounts[:T-1-t] * advs[t:]) for t in range(T-1)])\n",
    "\n",
    "        values = values[:-1,...]\n",
    "        discounts = torch.FloatTensor(discounts[:-1]).unsqueeze(1)\n",
    "        returns = torch.FloatTensor(returns[:-1]).unsqueeze(1)\n",
    "        gaes = torch.FloatTensor(gaes).unsqueeze(1)\n",
    "\n",
    "        # now use the gaes to calculate the policy loss\n",
    "        # And proceed as before\n",
    "        policy_loss = -(discounts * gaes.detach() * logpas).mean()\n",
    "        entropy_loss = -entropies.mean()\n",
    "        loss = policy_loss + self.entropy_loss_weight * entropy_loss\n",
    "        self.shared_policy_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(local_policy_model.parameters(), self.policy_model_max_grad_norm)\n",
    "        for param, shared_param in zip(local_policy_model.parameters(), self.shared_policy_model.parameters()):\n",
    "            if shared_param.grad is None:\n",
    "                shared_param._grad = param.grad\n",
    "        self.shared_policy_optimizer.step()\n",
    "        local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n",
    "\n",
    "        value_error = returns - values\n",
    "        value_loss = value_error.pow(2).mul(0.5).mean()\n",
    "        self.shared_value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(local_value_model.parameters(), self.value_model_max_grad_norm)\n",
    "        for param, shared_param in zip(local_value_model.parameters(), self.shared_value_model.parameters()):\n",
    "            if shared_param.grad is None:\n",
    "                shared_param._grad = param.grad\n",
    "        self.shared_value_optimizer.step()\n",
    "        local_value_model.load_state_dict(self.shared_value_model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def interaction_step(state, env, local_policy_model, local_value_model,\n",
    "                         logpas, entropies, rewards, values):\n",
    "        action, is_exploratory, logpa, entropy = local_policy_model.full_pass(state)\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "\n",
    "        logpas.append(logpa)\n",
    "        entropies.append(entropy)\n",
    "        rewards.append(reward)\n",
    "        values.append(local_value_model(state))\n",
    "\n",
    "        return new_state, reward, is_terminal, is_truncated, is_exploratory\n",
    "\n",
    "    def work(self, rank):\n",
    "        last_debug_time = float('-inf')\n",
    "        self.stats['n_active_workers'].add_(1)\n",
    "        \n",
    "        local_seed = self.seed + rank\n",
    "        env = self.make_env_fn(seed=local_seed)\n",
    "        torch.manual_seed(local_seed) \n",
    "        np.random.seed(local_seed)\n",
    "        random.seed(local_seed)\n",
    "\n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.n\n",
    "        \n",
    "        local_policy_model = self.policy_model_fn(nS, nA)\n",
    "        local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n",
    "        local_value_model = self.value_model_fn(nS)\n",
    "        local_value_model.load_state_dict(self.shared_value_model.state_dict())\n",
    "\n",
    "        global_episode_idx = self.stats['episode'].add_(1).item() - 1\n",
    "        while not self.get_out_signal:            \n",
    "            episode_start = time.time()\n",
    "            state, is_terminal = env.reset(), False\n",
    "            \n",
    "            # collect n_steps rollout\n",
    "            n_steps_start, total_episode_rewards = 0, 0\n",
    "            total_episode_steps, total_episode_exploration = 0, 0\n",
    "            logpas, entropies, rewards, values = [], [], [], []\n",
    "\n",
    "            for step in count(start=1):\n",
    "                state, reward, is_terminal, is_truncated, is_exploratory = self.interaction_step(\n",
    "                    state, env, local_policy_model, local_value_model, \n",
    "                    logpas, entropies, rewards, values)\n",
    "\n",
    "                total_episode_steps += 1\n",
    "                total_episode_rewards += reward\n",
    "                total_episode_exploration += int(is_exploratory)\n",
    "                \n",
    "                if is_terminal or step - n_steps_start == self.max_n_steps:\n",
    "                    is_failure = is_terminal and not is_truncated\n",
    "                    next_value = 0 if is_failure else local_value_model(state).detach().item()\n",
    "                    rewards.append(next_value)\n",
    "                    values.append(torch.FloatTensor([[next_value,],]))\n",
    "\n",
    "                    self.optimize_model(logpas, entropies, rewards, values, \n",
    "                                        local_policy_model, local_value_model)\n",
    "                    logpas, entropies, rewards, values = [], [], [], []\n",
    "                    n_steps_start = step\n",
    "                \n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "            # save global stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            evaluation_score, _ = self.evaluate(local_policy_model, env)\n",
    "\n",
    "            self.stats['episode_elapsed'][global_episode_idx].add_(episode_elapsed)\n",
    "            self.stats['episode_timestep'][global_episode_idx].add_(total_episode_steps)\n",
    "            self.stats['episode_reward'][global_episode_idx].add_(total_episode_rewards)\n",
    "            self.stats['episode_exploration'][global_episode_idx].add_(total_episode_exploration/total_episode_steps)\n",
    "            self.stats['evaluation_scores'][global_episode_idx].add_(evaluation_score)\n",
    "\n",
    "            mean_10_reward = self.stats['episode_reward'][:global_episode_idx+1][-10:].mean().item()\n",
    "            mean_100_reward = self.stats['episode_reward'][:global_episode_idx+1][-100:].mean().item()\n",
    "            mean_100_eval_score = self.stats['evaluation_scores'][:global_episode_idx+1][-100:].mean().item()\n",
    "            mean_100_exp_rat = self.stats['episode_exploration'][:global_episode_idx+1][-100:].mean().item()\n",
    "            std_10_reward = self.stats['episode_reward'][:global_episode_idx+1][-10:].std().item()\n",
    "            std_100_reward = self.stats['episode_reward'][:global_episode_idx+1][-100:].std().item()\n",
    "            std_100_eval_score = self.stats['evaluation_scores'][:global_episode_idx+1][-100:].std().item()\n",
    "            std_100_exp_rat = self.stats['episode_exploration'][:global_episode_idx+1][-100:].std().item()\n",
    "            if std_10_reward != std_10_reward: std_10_reward = 0            \n",
    "            if std_100_reward != std_100_reward: std_100_reward = 0\n",
    "            if std_100_eval_score != std_100_eval_score: std_100_eval_score = 0\n",
    "            if std_100_exp_rat != std_100_exp_rat: std_100_exp_rat = 0\n",
    "            global_n_steps = self.stats['episode_timestep'][:global_episode_idx+1].sum().item()\n",
    "            global_training_elapsed = self.stats['episode_elapsed'][:global_episode_idx+1].sum().item()\n",
    "            wallclock_elapsed = time.time() - self.training_start\n",
    "            \n",
    "            self.stats['result'][global_episode_idx][0].add_(global_n_steps)\n",
    "            self.stats['result'][global_episode_idx][1].add_(mean_100_reward)\n",
    "            self.stats['result'][global_episode_idx][2].add_(mean_100_eval_score)\n",
    "            self.stats['result'][global_episode_idx][3].add_(global_training_elapsed)\n",
    "            self.stats['result'][global_episode_idx][4].add_(wallclock_elapsed)\n",
    "\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - self.training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:06}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, global_episode_idx, global_n_steps, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "\n",
    "            if rank == 0:\n",
    "                print(debug_message, end='\\r', flush=True)\n",
    "                if time.time() - last_debug_time >= 60:\n",
    "                    print(ERASE_LINE + debug_message, flush=True)\n",
    "                    last_debug_time = time.time()\n",
    "\n",
    "            with self.get_out_lock:\n",
    "                potential_next_global_episode_idx = self.stats['episode'].item()\n",
    "                self.reached_goal_mean_reward.add_(mean_100_eval_score >= self.goal_mean_100_reward)\n",
    "                self.reached_max_minutes.add_(time.time() - self.training_start >= self.max_minutes * 60)\n",
    "                self.reached_max_episodes.add_(potential_next_global_episode_idx >= self.max_episodes)\n",
    "                if self.reached_max_episodes or \\\n",
    "                   self.reached_max_minutes or \\\n",
    "                   self.reached_goal_mean_reward:\n",
    "                    self.get_out_signal.add_(1)\n",
    "                    break\n",
    "                # else go work on another episode\n",
    "                global_episode_idx = self.stats['episode'].add_(1).item() - 1\n",
    "\n",
    "        while rank == 0 and self.stats['n_active_workers'].item() > 1:\n",
    "            pass\n",
    "\n",
    "        if rank == 0:\n",
    "            print(debug_message)\n",
    "            if self.reached_max_minutes: print(u'--> reached_max_minutes')\n",
    "            if self.reached_max_episodes: print(u'--> reached_max_episodes')\n",
    "            if self.reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "\n",
    "        env.close() ; del env\n",
    "        self.stats['n_active_workers'].sub_(1)\n",
    "\n",
    "\n",
    "    def train(self, make_env_fn, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        self.max_minutes = max_minutes\n",
    "        self.max_episodes = max_episodes\n",
    "        self.goal_mean_100_reward = goal_mean_100_reward\n",
    "\n",
    "        env = self.make_env_fn(seed=self.seed)\n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.n\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        self.stats = {}\n",
    "        self.stats['episode'] = torch.zeros(1, dtype=torch.int).share_memory_()\n",
    "        self.stats['result'] = torch.zeros([max_episodes, 5]).share_memory_()\n",
    "        self.stats['evaluation_scores'] = torch.zeros([max_episodes]).share_memory_()\n",
    "        self.stats['episode_reward'] = torch.zeros([max_episodes]).share_memory_()\n",
    "        self.stats['episode_timestep'] = torch.zeros([max_episodes], dtype=torch.int).share_memory_()\n",
    "        self.stats['episode_exploration'] = torch.zeros([max_episodes]).share_memory_()\n",
    "        self.stats['episode_elapsed'] = torch.zeros([max_episodes]).share_memory_()\n",
    "        self.stats['n_active_workers'] = torch.zeros(1, dtype=torch.int).share_memory_()\n",
    "\n",
    "        self.shared_policy_model = self.policy_model_fn(nS, nA).share_memory()\n",
    "        self.shared_policy_optimizer = self.policy_optimizer_fn(self.shared_policy_model, self.policy_optimizer_lr)\n",
    "        self.shared_value_model = self.value_model_fn(nS).share_memory()\n",
    "        self.shared_value_optimizer = self.value_optimizer_fn(self.shared_value_model, self.value_optimizer_lr)\n",
    "\n",
    "        self.get_out_lock = mp.Lock()\n",
    "        self.get_out_signal = torch.zeros(1, dtype=torch.int).share_memory_()\n",
    "        self.reached_max_minutes = torch.zeros(1, dtype=torch.int).share_memory_() \n",
    "        self.reached_max_episodes = torch.zeros(1, dtype=torch.int).share_memory_() \n",
    "        self.reached_goal_mean_reward  = torch.zeros(1, dtype=torch.int).share_memory_() \n",
    "        self.training_start = time.time()\n",
    "        workers = [mp.Process(target=self.work, args=(rank,)) for rank in range(self.n_workers)]\n",
    "        [w.start() for w in workers] ; [w.join() for w in workers]\n",
    "        wallclock_time = time.time() - self.training_start\n",
    "\n",
    "        final_eval_score, score_std = self.evaluate(self.shared_policy_model, env, n_episodes=100)\n",
    "        env.close() ; del env\n",
    "\n",
    "        final_episode = self.stats['episode'].item()\n",
    "        training_time = self.stats['episode_elapsed'][:final_episode+1].sum().item()\n",
    "\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "\n",
    "        self.stats['result'] = self.stats['result'].numpy()\n",
    "        self.stats['result'][final_episode:, ...] = np.nan\n",
    "        return self.stats['result'], final_eval_score, training_time, wallclock_time\n",
    "\n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1, greedy=True):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                if greedy:\n",
    "                    a = eval_policy_model.select_greedy_action(s)\n",
    "                else: \n",
    "                    a = eval_policy_model.select_action(s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pathos.multiprocessing' has no attribute 'Lock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/ipykernel_32576/3679065445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 entropy_loss_weight, max_n_steps, n_workers, tau)\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     result, final_eval_score, training_time, wallclock_time = agent.train(make_env, seed, gamma, \n\u001b[0m\u001b[1;32m     32\u001b[0m                                                                           \u001b[0mmax_minutes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                                           goal_mean_100_reward)\n",
      "\u001b[0;32m/var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/ipykernel_32576/1794308898.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, make_env_fn, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_value_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_optimizer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_value_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_optimizer_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_out_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_out_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_memory_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreached_max_minutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_memory_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pathos.multiprocessing' has no attribute 'Lock'"
     ]
    }
   ],
   "source": [
    "gae_results = []\n",
    "best_gae_agent, best_gae_eval_score = None, float('-inf')\n",
    "\n",
    "gamma = 0.99\n",
    "max_minutes = 10\n",
    "max_episodes = 10000\n",
    "goal_mean_100_reward = 475\n",
    "\n",
    "for seed in (12, 34, 56, 78, 90):\n",
    "    \n",
    "    policy_model_fn = lambda nS, nA: FCR(nS, nA, hidden_dims=(128,64))\n",
    "    policy_model_max_grad_norm = 1\n",
    "    policy_optimizer_fn = lambda net, lr: SharedAdam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0005\n",
    "\n",
    "    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,128))\n",
    "    value_model_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: SharedRMSprop(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0007\n",
    "\n",
    "    entropy_loss_weight = 0.001\n",
    "\n",
    "    max_n_steps = 50\n",
    "    n_workers = 8\n",
    "    tau = 0.95\n",
    "\n",
    "    agent = GAE(policy_model_fn, policy_model_max_grad_norm, policy_optimizer_fn, policy_optimizer_lr,\n",
    "                value_model_fn, value_model_max_grad_norm, value_optimizer_fn, value_optimizer_lr, \n",
    "                entropy_loss_weight, max_n_steps, n_workers, tau)\n",
    "\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(make_env, seed, gamma, \n",
    "                                                                          max_minutes, max_episodes, \n",
    "                                                                          goal_mean_100_reward)\n",
    "  \n",
    "    gae_results.append(result)\n",
    "    if final_eval_score > best_gae_eval_score:\n",
    "        best_gae_eval_score = final_eval_score\n",
    "        best_gae_agent = agent\n",
    "gae_results = np.array(gae_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage actor-critic (A2C)\n",
    "\n",
    "Advantage actor-critic (A2C) is the synchronous version of A3C. Updating the neural network in a Hogwild!-style can be chaotic, yet introducing a lock mechanism lowers A3C performance considerably. In A2C, we move the workers from the agent\n",
    "down to the environment. Instead of having multiple actor-learners, we have multiple actors with a single learner.\n",
    "\n",
    "<img src=\"./images/synchronous-model.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a single neural network for both the policy andcthe value function. Sharing a model can be particularly beneficial when learning from images, because feature extraction can be compute-intensive. However, model sharing can be challenging due to the potentially different scales of the policy and value function updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "\n",
    "class test_multi():\n",
    "    def __init__(self, n_workers):\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "    def work(self, rank):\n",
    "        print(rank)\n",
    "        \n",
    "    def train(self):\n",
    "        print('start')\n",
    "        self.get_out_lock = mp.Lock()\n",
    "        workers = [mp.Process(target=self.work, args=(rank,)) for rank in range(self.n_workers)]  \n",
    "        [w.start() for w in workers]\n",
    "        [w.join() for w in workers]\n",
    "        print('end')\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "01\n",
      "\n",
      "2\n",
      "3\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "test = test_multi(n_workers=4)\n",
    "test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "math_differential_calculus",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8aeb84091b1f1fb8d8b9efbf1e96a552fa0144c39bfbc7f744113ad2216f701d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
