{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based RL and Frozen Lake problem\n",
    "\n",
    "Apply DynaQ and Trajectory sampling agents on the Frozen Lake (FL) environment using a discount factor gamma=0.99. Then compare results with SARSA(lambda) and Q(lambda).For the comparison, you can plot only states (0, 6, 10). \n",
    "\n",
    "Remember: FL is a simple grid-world environment. It has 16 states and 4 actions. The goal of the agent is to go from a start location to a goal location while avoiding falling into holes. All transitions landing on the goal state  provide a +1 reward, while every other transition in the entire grid world provides no reward. The challenge is that the surface of the lake is frozen, and therefore slippery. So actions have stochastic effects, and the agent moves only a third of the time as intended. The other two-thirds is split evenly in orthogonal directions.\n",
    "\n",
    "FL is a more challenging environment than Slippery Walk environment. Therefore, one of the most important changes you need to make is to increase the number of episodes the agent interacts with the environment. While in the Slippery Walk environment we allow the agent to interact for only 2.000 episodes, in the FL environment, let your agent gather experience for 10.000 episodes.\n",
    "\n",
    "Optimal values are (as already calculated using DP in the previous lab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_V = [0.542, 0.4988, 0.4707, 0.4569,\n",
    "             0.5585, 0.0, 0.3583, 0.0, \n",
    "             0.5918, 0.6431, 0.6152, 0.0,\n",
    "             0.0, 0.7417, 0.8628, 0.0 ]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Import the FrozenLake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# You can get the environment from Gymnasium 'FrozenLake-v1’; \n",
    "# In order to visually plot the environment you can import it \n",
    "# using render_mode=\"rgb_array\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Implement the DynaQ algorithm and use it against the FL environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Copy the decay_schedule function from the lecture notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Copy the select_action function from the lecture notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Copy the dyna_q function from the lecture notes and modify it in order to ask the environment the number\n",
    "# of states and actions following the Gymnasium interface (.n), to call the step() function using \n",
    "# the Gymnasium interface (5 outputs), and extract the id of the starting state from the first \n",
    "# output of the reset() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Apply DynaQ for 10.000 episodes and gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Plot the state-value functions for states (0, 6, 10) for DynaQ and observe that the agent is even faster than the Q(lambda) agent at tracking the true values, but notice too, how there’s a large error spike at the beginning of training. This is likely because the model is incorrect early on, and Dyna-Q randomly samples states from the learned model, even states not sufficiently visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Import matplotlib.pyplot as plt and plot the V_track_dq for each episode for states 0, 6 and 10\n",
    "# draw a horizontal line for the optimal_V for states 0, 6 and 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Implement the Trajectory Sampling algorithm and use it against the FL environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Copy the trajectory_sampling function from the lecture notes and modify it in order to ask the environment the number\n",
    "# of states and actions following the Gymnasium interface (.n), to call the step() function using \n",
    "# the Gymnasium interface (5 outputs), and extract the id of the starting state from the first \n",
    "# output of the reset() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Apply trajectory_sampling for 10.000 episodes ans gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Plot the state-value functions for states (0, 6, 10) for Trajectory Sampling and observe that there is more stability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Plot the V_track_ts for each episode for states 0, 6 and 10\n",
    "# draw a horizontal line for the optimal_V for states 0, 6 and 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - compare all methods (also SARSA(lambda) and Q(lambda)) plotting the mean absolute error and observe that the model-based methods also bring the error down close to zero. However, shortly after 2,000 episodes, both model-based and Q(lambda) methods are much the same. SARSA(lambda) methods are also slow to optimal here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Copy the SARSA(lambda) function from the previous lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Apply SARSA(lambda) for 10.000 episodes ans gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Copy the Q(lambda) function from the previous lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Apply Q(lambda) for 10.000 episodes ans gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Copy the moving average function from the previous lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# copy the plotting of the moving average of the absolute error \n",
    "# from the previous lab\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "math_differential_calculus",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8aeb84091b1f1fb8d8b9efbf1e96a552fa0144c39bfbc7f744113ad2216f701d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
